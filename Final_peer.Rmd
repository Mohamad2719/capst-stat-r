---
title: "Peer Assessment II"
output:
  html_document: 
    pandoc_args: [
      "--number-sections",
    ]
---

# Background

As a statistical consultant working for a real estate investment firm, your task is to develop a model to predict the selling price of a given home in Ames, Iowa. Your employer hopes to use this information to help assess whether the asking price of a house is higher or lower than the true value of the house. If the home is undervalued, it may be a good investment for the firm.

# Training Data and relevant packages

In order to better assess the quality of the model you will produce, the data have been randomly divided into three separate pieces: a training data set, a testing data set, and a validation data set. For now we will load the training data set, the others will be loaded and used later.

```{r load, message = FALSE}
load("ames_train.Rdata")
load("ames_test.Rdata")
load("ames_validation.Rdata")
```

Use the code block below to load any necessary packages

```{r packages, message = FALSE}
library('plyr')         # data manipulation
library('ggplot2')      # library to create plots
library('dplyr')        # data manipulation
library('knitr')        # required to apply knitr options 
library('statsr')       # staistics functions   

library('GGally')       # library to create plots
library('gridExtra')    # arrange plots
library('BAS')          # Bayesian statistics functions
library('labeling')     # dependency package to ggplot

library('MASS')         # Support Functions 
library('car')          # Companion to Applied Regression

library('corrplot')     # correlation plot
library('RColorBrewer') # Color scheme 

source('helper.R')      # support functions used in the analysis - See Appendix

# apply general knitr options
knitr::opts_chunk$set(comment=NA, fig.align='center')
```


```{r}
data.train <- ames_train
data.test <- ames_test
data.validation <- ames_validation
rm(ames_test, ames_train, ames_validation)
```


## Part 1 - Exploratory Data Analysis (EDA)

**Summarize Missing values**

We initially start verifying the missing values in our data set. This is just an initial step to define, which feature needs maintenance.

```{r}
# check which features contain NA values
CheckNA(data.train, 2)
```

**Strategy for missing values**

Handling missing values is an important part of the data cleaning. The chosen approach is not meant to be a highly sophisticated approach to fill the missing values, but will fulfil it's duty. The codebook reveals that for multiple features `NA` value have been applied, if an item doesn't exist (e.g. Garage.Cars -> NA means no garage). The original dataset also contains empty values. We'll transform the empty values into `NA's`.
In case of a numerical feature we apply a `0` value, for categorical features (e.g. `poor`, `fair`, `typical/average`, `good` and `excellent`), we'll map numeric values (e.g. `0 - 5`) to their corresponding categoric values (including `0` for none) and combine those with our dataset. But there are also categorical features where an `NA` placeholder will get a value assigned. The assigned value has been evaluated based on the majority of occurrences. Find the list below.

**Categorical features containing an `NA` value will be set to `none`, and mapped to a defined value**

Feature `Paved.Drive` set to **1** parameter `Grvl`
Feature `Street` set to **1** parameter `Grvl`
Feature `Lot.Config` set to **1** parameter `Inside`
Feature `MS.Zoning` set to **1** parameter `A(all)`
Feature `Land.Slope` set to **1** parameter `Gtl`
Feature `Lot.Shape` set to **1** parameter `Reg`
Feature `Land.Contour` set to **1** parameter `Lvl`
Feature `Sale.Condition` set to **1** parameter `Normal`
Feature `Sale.Type` set to **10** parameter `WD`
Feature `Electrical` set to **5** parameter `SBrkr`
Feature `Heating` set to **2** parameter `GasA`
Feature `Foundation` set to **3** parameter `Pconc`
Feature `Exterior.1st` set to **15** parameter `VinylSD`
Feature `Exterior.2nd` set to **15** parameter	`VinylSD`
Feature `Condition.1 / Condition.2` set to **3** parameter `Norm`
Feature `Roof.style` set to **2** parameter `Gable`
Feature `Bldg.Type` set to **1** parameter `1Fam`
Feature `House.Style` set to **1** parameter `1Story`
Feature `Neighborhood` set to **1** parameter `Blmngtn`

**Categorical features containing an `NA` value will be set to `none` and mapped to a 0 value**

`Garage.type`, `Garage.Finish`, `Garage.Qual`, `Garage.Cond`, `Pool.QC`,
`Fireplace.Qu`, `Fence`, `Misc.Feature`, `Alley`, `Bsmt.Qual`, `Bsmt.Cond`,
`BsmtFin.Type.1`, `Bsmt.Exposure`, `BsmtFin.Type.2`


**Numerical features containing an `NA` will be set to 0**

`Garage.Cars`, `Garage.Area`, `Garage.Yr.Blt`, `BsmtFin.SF.1`, `BsmtFin.SF.2`, `Bsmt.Unf.SF`, `Total.Bsmt.SF`, `Bsmt.Full.Bath`, `Bsmt.Half.Bath`, `Mas.Vnr.Area`


```{r}
# Clean empty fields by adding an NA vaue. Then check for NA values and
# run filling in values for missing data 

data.train <- cleanEmptyFields(data.train, NA)
data.train <- NaHandler(data.train)
CheckNA(data.train, 0)             # This check should reveil no NA values
data.train <- recode.features(data.train)

# NA value handling test data
data.test <- cleanEmptyFields(data.test, NA)
data.test <- NaHandler(data.test)
CheckNA(data.test, 0)              # This check should reveil no NA values
data.test <- recode.features(data.test)

# NA value handling validation data
data.validation <- cleanEmptyFields(data.validation, NA)
data.validation <- NaHandler(data.validation)
CheckNA(data.validation, 0)        # This check should reveil no NA values
data.validation <- recode.features(data.validation)
```

**Normalize Data set**

When dealing with values that lie in different ranges, it is common to normalize the data for better evaluation. Here, I used the min and max normalization approach.

```{r}
# Normalize training data set and store a original data set
df.train.normalized <- NormalizeDF(data.train, 'price')
df.train.normalized.org <- df.train.normalized

# Normalize test data set and store a original data set
df.test.normalized <- NormalizeDF(data.test, 'price')
df.test.normalized.org <- df.test.normalized

# Normalize validation data set and store a original data set
data.validation <- data.validation[complete.cases(data.validation), ]
df.validation.normalized <- NormalizeDF(data.validation, 'price')
df.validation.normalized.org <- df.validation.normalized
```

```{r}
# create working copies
df.train      <- data.train
df.test       <- data.test
df.validation <- data.validation

# final Clean Up
df.train      <- finalCleanUp(df.train)
df.test       <- finalCleanUp(df.test)
df.validation <- finalCleanUp(df.validation)

# create an original with and without outliners
df.train.Outliner   <- df.train
df.train.NoOutliner <- removeOutliners(df.train, c(428, 310))
```


**Correlation Matrix**

I've transformed all the categoric features with an ordinal scale into numeric columns. In order to determine the feature with the strongest relation towards price, we need to calculate the sample correlation coefficient between two variables.

We're particularly interested in variables that show a strong relation to price, so we will focus primarily on features that have a coefficient > 0.5 or < -0.5.

```{r}
correlations = cor(df.train, method = "s")

# only want the columns that show strong correlations with price
corr.price = as.matrix(sort(correlations[,'price'], decreasing = TRUE))

corr.idx = names(which(apply(corr.price, 1, function(x) (x > 0.5 | x < -0.5))))

corrplot(as.matrix(correlations[corr.idx,corr.idx]), type = 'upper', method='color', 
         title = 'Correlation Plot of the 20 features', 
         addCoef.col = 'black', tl.cex = .7,cl.cex = .7, number.cex=.7)
```

The correlation plot indicates the 20 features with the strongest effect on housing prices. I won't go through all of them, but pick a few to comment on.

The features `Overall.Qual` and `area` show the strongest correlation. I reckon this makes sense, since the overall quality of a house must have an impact on the sales price. With `Garage.Cars`, `Year.Built` we ceratinly could expect those features to be important. Further we have the features with a quality aspect `Bsmt.Qual`, `Exter.Qual`, `Kitchen.Qual`, which I believe might be relevant. In general, all quality related features might have an impact.

I wouldn't expect `Garage.Yr.Blt` to be an important feature, but let it be so. `YearBuilt`, `YearRemodAdd` a positive correlation of those features should be **NO** surprise to us. Newer homes will likely sell for higher prices then older ones.

```{r}
df.train <- df.train %>% dplyr::select(price, X1st.Flr.SF, 
                                       X2nd.Flr.SF, Lot.Area, 
                                       MS.SubClass, Mas.Vnr.Type, 
                                       Mas.Vnr.Area, Garage.Area, 
                                       Screen.Porch, Year.Built, 
                                       BsmtFin.SF.1, Pool.Area, 
                                       TotRms.AbvGrd)


ggpairs(df.train, 
          columns=1:13, 
          upper = list(continuous = wrap('cor', size = 4, col = 'steelblue')), 
          diag  = list(continuous = ggpairs_diag),
          lower = list(continuous = ggpairs_lower), 
          title = 'Correlation & Density plot'
          ) 
```


This graph shows multiple information in a compact way. The main required information is to see the distribution of the given features, if we need to potentially transform the feature at a latter stage. It also shows the general correlation and some indication of outliners in a compact form. This plot is not meant to give detailed information. It is meant to be used to isolate some trends, potential problems etc. to give a hint where deeper investigation is required.

What we see here is, price is right skewed and not normal distributed, or that the feature `Mas.Vnr.Area` has some Outliners or high leverage points. Those findings will then require a deeper analysis.


```{r}
summary.Price <- df.train %>% dplyr::summarise(mean_Price   = mean(price),
                                    median_Price = median(price),
                                    sd_Price     = sd(price),
                                    min_Price    = min(price),
                                    max_Price    = max(price),
                                    IQR_Price    = IQR(price),
                                    total = n())
```


```{r}
kable(summary.Price[1:6], caption = 'Price table of Houses in Ames Iowa')
```

The table above just shows a summary of the sales prices of homes in Ames, Iowa.

* * *

## Part 2 - Development and assessment of an initial model, following a semi-guided process of analysis

### Section 2.1 An Initial Model

Running through the EDA and the codebook, I figured that the features, which are related to size, quality and condition would be major drivers for the sales price of a house. Therefore I came up with the following initial "intuitive" model.

Initial Model Selection
**price ~ area + Overall.Qual + Overall.Cond + Kitchen.Qual + Bsmt.Qual + Exter.Qual + Year.Built + Garage.Area + Lot.Area + Fireplace.Qu**

Next we show a summary of our initial model.

```{r}
# 2.2.1 Initial Model selection 
df.train <- df.train.Outliner

df.train <- df.train %>% dplyr::select(price, area, Overall.Qual, 
                                                       Overall.Cond, Kitchen.Qual, 
                                                       Year.Built, Bsmt.Qual, 
                                                       Exter.Qual, Garage.Area, 
                                                       Lot.Area, Fireplace.Qu)

initial.model  <- as.formula(price ~ area + Overall.Qual + 
                               Overall.Cond + Year.Built + 
                               Exter.Qual +  Bsmt.Qual + 
                               Lot.Area + Kitchen.Qual + 
                               Fireplace.Qu + Garage.Area)

initial.lm.model <- lm(initial.model , data = df.train)
summary(initial.lm.model)
```

Summary estimates should only be trusted if the conditions for the regression are reasonable. Having mentioned that, it will be relevant to validate the following aspects:

**Evaluate conditions for Regression**

There is a linear relationship between any numerical predictor variables and the response variable.

1-The residuals are nearly normal distributed
2-The residuals display constant variability
3-The residuals are independent

We first will examine whether the numerical variables included in the model, are linearly related to the response variable `price` by examining the distribution of the residuals.

```{r}
ggplot(data = NULL, aes(x = df.train$price, y = initial.lm.model$residuals)) + 
  geom_point(col = 'darkgreen') + 
  geom_hline(yintercept = 0, linetype = 'dashed', color = 'red')
```

The residuals are scattered randomly around 0 in most cases except for the ones in the higher price segment. In general the residuals exhibit "heteroscedasticity", meaning that the residuals get larger as the prediction moves from small to large. This is something we need to keep in mind. Next, we check whether the residuals display a nearly normal distribution centred around 0.

```{r}
par(mfrow = c(1,2))
hist(initial.lm.model$residuals, col = 'steelblue')
qqnorm(initial.lm.model$residuals, col = 'darkgreen')
qqline(initial.lm.model$residuals, col = 'red')
```

The results of the histogram of the residuals shows a normal distribution around 0, which is slightly left skewed. This might be driven by an 'extreme' outliners (see figure below). The Q-Q plot also indicates some skewness in the tails, but there are no major deviations. So we can conclude that the conditions for this model are reasonable.

```{r}
par(mfrow = c(1,2))
ggplot(data = NULL, aes(x = initial.lm.model$fitted, y = initial.lm.model$residuals)) + 
  geom_point(col = 'darkgreen') + 
  geom_hline(yintercept = 0, linetype = 'dashed', color = 'red')
```

The results show that the residuals are equally variable for low and medium values of the predicted values, i.e., residuals have a constant variability, while in higher price segments the model tends to over estimate. We can also perform a hierarchical analysis of variance table by using the anova function: An ANOVA table shows the possible associations between the independent and dependent variables. The results of the ANOVA test are shown below.

```{r}
anova(initial.lm.model)
```

With regards to inference for the model, the p-value of the model's F-statistic indicates that the model as a whole is significant, except for  `Overall.Cond`. We need to consider removing this feature.

### Section 2.2 Model Selection

We will now consider three different model selection types based on our initial model and verify if they will arrive at the same parsimonious model.

1-P-Value (find the summary statistics at the beginning of this section 2.1)
2-Akaike information criterion (AIC)
3-Bayesian Information Criterion (BIC)


```{r}
# Model selection using Akaike information criterion (AIC)

lm.model <- lm(initial.model,
               data = df.train)

model.AIC <- stepAIC(lm.model, k = 2, trace = FALSE)
model.AIC$anova
```


```{r}
# Model selection using Bayesian Information Criterion (BIC)

houses.BIC = bas.lm(initial.model,
                    prior = "BIC",
                    modelprior = uniform(),
                    data = df.train)

image (houses.BIC, rotate = FALSE)
```


What we see from the analysis is, that all three model selection methods end up with the same result. The initial model provides p-values, which indicate significance for all chosen features. It isn't really surprising that all approaches came to the same conclusion. I chose features, which I personally would rely on, if I needed to select a house. But anyway, even if all features are relevant, doesn't per se mean that the model as such is good. The chosen features were intuitively chosen and are all important.

* * *

### Section 2.3 Initial Model Residuals

One way to assess the performance of a model is to examine the model's residuals. Below, you'll find a residual plot as well as the applied regression line, which indicates the linearity of the data.


```{r}
ggplot(data = NULL, aes(x = initial.lm.model$fitted, y = initial.lm.model$residuals)) + 
  geom_point(col = 'darkgreen') + 
  geom_smooth(method = "lm", formula = y ~ splines::bs(x, 3), se = TRUE) + 
  geom_hline(yintercept = 0, linetype = 'dashed', color = 'red')
```

The plot shows that the residuals are equally variable for medium values of the predicted values, i.e., residuals have a constant variability, while in lower and higher price segments the model tends to over estimate. The regression line shows the general trend of the distribution. It also exhibits "heteroscedasticity", meaning that the residuals get larger as the prediction moves from small to large. There are ways to resolve this. The most successful solution to overcome heteroscedasticity is to transform a variable, or heteroscedasticity often indicates that an important variable is missing. We will use the outcome of this curve in section 3.2 Transformation and 3.3 Variable Interaction, where we decide if we need to apply any type of interaction or transformation. But it right away indicates a quadratic behaviour, which we certainly need to consider.

### Section 2.4 Initial Model RMSE

We will now calculate the Root Mean Squared Error based on the model output.

When we fit a linear regression to a specific data set, many problems may occur. Most common among those are the following: 
1) Non linearity of the response - predictor
2) Correlation of error terms
3) Non constant variance of error terms
4) Outliners
5) High leverage point
6) Collinearity

All of the listed factors have an influence on the RMSE of the model. Therefore we will investigate on a few of them. The influence for some of these aspects can be determined from graph below. I won't go through all of them, but certainly address a few.

**1) Non linearity from plot below**
The residual vs. fitted plot (top left) shows a clear non linearity in the residuals. This leads to the conclusion that we need to consider a non linear function.

**2) outliners**
The residual vs. fitted plot (top left) also shows potential outliners marked by numbers. Removing outliners should be carefully considered. With a leverage effect one can create a situation where 1% of your data points affect the slope, which might affect 50% of our data points. I think the best way to start is to ask whether the outliers even make sense, especially given the other variables we have in our data set.

**3) High leverage point**
An influential point is one if removed from the data would significantly change the fit. An influential point may either be an outlier or have large leverage, or both, but it will tend to have at least one of those properties. Cook's distance is a commonly used influence measure that combines these two properties. The Residual vs. Leverage plot depicts the cook's distance and all the potential leverage points within the boundaries. As we can see, there are no **high leverage points** in our data.

Let's run an initial analysis by verifying outliners and their impact on the adjusted R2 and RMSE. I'll investigate the impact on adjusted R2 and RMSE by applying a log transformation on the response variable as well.

```{r}
predicted <- predict(initial.lm.model, df.train)

RMSE.w_outliner <- rmse(df.train$price, predicted)
adj.r.squared.w_outliner <- summary(initial.lm.model)$adj.r.squared

# Initial Model selection log(price)

formula.log <- as.formula(log(price) ~ area + Overall.Qual + Overall.Cond + 
                            Kitchen.Qual + Year.Built + Bsmt.Qual + Exter.Qual + 
                            Garage.Area + Lot.Area + Fireplace.Qu)

initial.lm.model.log <- lm(formula.log , data = df.train)
adj.r.squared.log.w_outliner <- summary(initial.lm.model.log)$adj.r.squared

predicted.log <- predict(initial.lm.model.log, df.train)
RMSE.log.w_outliner <- rmse(df.train$price, exp(predicted.log))

# This part verifies the impact of the most severe outliners. 
# So we check for outliners. We remove the outliner and re run model
# Afterwards we compare the RMSE and adjusted R Squared. 

df.train <- df.train.NoOutliner

# re run model

initial.lm.model <- lm(initial.model , data = df.train)
adj.r.squared.wo_outliner <- summary(initial.lm.model)$adj.r.squared

predicted <- predict(initial.lm.model, df.train)
RMSE.wo_outliner <- rmse(df.train$price, predicted)

# Initial Model selection log(price)

initial.lm.model.log <- lm(formula.log , data = df.train)
adj.r.squared.log.wo_outliner <- summary(initial.lm.model.log)$adj.r.squared

predicted.log <- predict(initial.lm.model.log, df.train)
RMSE.log.wo_outliner <- rmse(df.train$price, exp(predicted.log))

with_outliner <- c(adj.r.squared.w_outliner, RMSE.w_outliner)
without_outliner <- c(adj.r.squared.wo_outliner, RMSE.wo_outliner)
log.with_outliner <- c(adj.r.squared.log.w_outliner, RMSE.log.w_outliner)
log.without_outliner <- c(adj.r.squared.log.wo_outliner, RMSE.log.wo_outliner)  
R_Squared_RMSE <- data.frame(rbind(with_outliner, without_outliner, log.with_outliner, log.without_outliner))

colnames(R_Squared_RMSE) <- c('adj_R_Squared', 'RMSE')

kable(R_Squared_RMSE[1:2], caption = 'adjusted R-Squared and RMSE')
```


Comparing the adjusted R2 as well as the RMSE as a quality measure, indicate that removing outliners or the transformation of features shows an improvement of the model. The Error of predicting housing prices can be reduced from **34'971.32** Dollars down to **28'026.33** Dollars. But, in this case we used the training data to predict housing prices and not the test-, or validation data.


### Section 2.5 Overfitting 

The process of building a model generally involves starting with an initial model, identifying its shortcomings, and adapting the model accordingly. This process may be repeated several times until the model fits the data reasonably well. However, the model may do well on training data but perform poorly out-of-sample (meaning, on a dataset other than the original training data) because the model is overly-tuned to specifically fit the training data. This is called overfitting. To determine whether overfitting is occurring on our model, we will compare the performance of our model on both in-sample and out-of-sample data sets.

**Remark:** To overcome Overfitting we might also apply a regularization term (L1 or L2) via LASSO or a Ridge regression. This proofed to be helpful and is even easier to apply then getting additional training data or optimizing our model. Regularization allows to play around with a Lambda term to overcome high bias or variance.

```{r loadtest, message = FALSE}
df.train <- df.train.NoOutliner

initial.model  <- as.formula(price ~ area + Overall.Qual + 
                               Overall.Cond + Year.Built + 
                               Exter.Qual +  Bsmt.Qual + 
                               Lot.Area + Kitchen.Qual + 
                               Fireplace.Qu + Garage.Area)

initial.lm.model <- lm(initial.model , data = df.train)

predicted.train <- predict(initial.lm.model, df.train)
RMSE.train <- sqrt( mean((df.train$price - predicted.train)^2))

predicted.test <- predict(initial.lm.model, df.test)
RMSE.test <- sqrt( mean((df.test$price - predicted.test)^2))

# log transform feature area
initial.model.log  <- as.formula(log(price) ~ log(area) + 
                                   Overall.Cond + Overall.Qual + 
                                   Year.Built + Exter.Qual +  Bsmt.Qual + 
                                   Lot.Area + Kitchen.Qual + 
                                   Fireplace.Qu + Garage.Area)

initial.lm.model.log <- lm(initial.model.log , data = df.train)

predicted.train <- predict(initial.lm.model.log, df.train)
RMSE.train.log <- sqrt( mean((df.train$price - exp(predicted.train))^2))

predicted.test <- predict(initial.lm.model.log, df.test)
RMSE.test.log <- sqrt( mean((df.test$price - exp(predicted.test))^2))

Initial.Model <- c(RMSE.train, RMSE.test)
Initial.Model.log <- c(RMSE.train.log, RMSE.test.log)

RMSE.Comp <-data.frame( rbind(Initial.Model, Initial.Model.log))
colnames(RMSE.Comp) <- c('RMSE on train', 'RMSE on test')

kable(RMSE.Comp[1:2], caption = 'RMSE on training and test data')
```

The initial model shows a difference between prediction with training data vs. prediction with test data. The model responds pretty well with the test data. This implies that there is no overfitting of the model. Applying some feature transformation also added some advantage as shown in the table.


## Part 3 Development of a Final Model

In the following section we will create a final model with around 20 variables to predict housing prices in Ames, IA, selecting from the full array of variables in the dataset by using the previously applied methods.

### Section 3.1 Final Model

The best model is not always the most complicated. Sometimes including variables that are not evidently important, can actually reduce the accuracy of predictions. In practice, the model that includes all available explanatory variables is often referred to as the full model. The full model may not be the best model, and if it isn't, we want to identify a smaller model that is preferable.

**Linear Model stepAIC Evaluation**

```{r}
# Model selection using AIC
df.train <- df.train.NoOutliner

lm.model <- lm(price ~ . ,
               data = df.train)

model.AIC <- stepAIC(lm.model, k = 2, trace = FALSE)
model.AIC$anova
```

```{r}
# Final Model: AIC 
Confirmed.model  <- as.formula(price ~ MS.SubClass + Lot.Area + Land.Slope + 
                                 Neighborhood + Condition.2 + Overall.Qual + 
                                 Overall.Cond + Year.Built + Exterior.1st + 
                                 Exterior.2nd + Mas.Vnr.Type + Mas.Vnr.Area + 
                                 Exter.Qual + Exter.Cond + Bsmt.Qual + 
                                 Bsmt.Cond + Bsmt.Exposure + BsmtFin.SF.1 + 
                                 BsmtFin.SF.2 + Bsmt.Unf.SF + Heating.QC + 
                                 X1st.Flr.SF + X2nd.Flr.SF + 
                                 Bsmt.Full.Bath + Bedroom.AbvGr + Kitchen.AbvGr + 
                                 Kitchen.Qual + TotRms.AbvGrd + Fireplaces + 
                                 Garage.Type + Garage.Finish + 
                                 Garage.Area + Garage.Qual + Open.Porch.SF + 
                                 Screen.Porch + Pool.Area + Pool.QC + Sale.Type + 
                                 Sale.Condition)
```


**Bayesian Model Averaging (BMA)**
A comprehensive approach to address model uncertainty is Bayesian model averaging, which allows us to assess the robustness of results to alternative specifications by calculating posterior distributions over coefficients and models. Given the 81 features (n) there can be 2^n = 2^81 possible models. We will explore model uncertainty using posterior probabilities of models based on BIC. We will use BIC as a way to approximate the log of the marginal likelihood. The Bayesian information criterion (BIC) runs through several fitted model objects for which a log-likelihood value can be obtained, according to the formula -2log-likelihood + nparlog(nobs), where npar represents the number of parameters and nobs the number of observations in the fitted model.

We show the model selection based on Bayesian Information Criterion.


```{r}
# Bayesian Information Criterion
model.BIC = bas.lm(Confirmed.model,
                    prior = "BIC",
                    modelprior = uniform(),
                    data = df.train)

image (model.BIC, rotate = FALSE)
```

We have now features identified with each of the three model selection approaches. Each shows a different set of best features. I'll not go into to much details and try to figure out, which might be the best proposed model. I'll rather just decide to continue with the proposed model via Bayesian Model Averaging (BMA).

I will now tune the model a bit, even if the model has been proposed via BMA model selection. I removed the Pool features, since those might be useless. The quality of the model will then be verified via Root Mean Squared Error on the test dataset.

```{r}
# Final model based on above Model Selection
Final.Model <- as.formula(price ~ Lot.Area + 
                          MS.SubClass   + Overall.Qual + 
                          Overall.Cond  + Year.Built + 
                          Mas.Vnr.Type  + Mas.Vnr.Area + 
                          Exter.Qual    + Exter.Cond + 
                          Bsmt.Exposure + BsmtFin.SF.1 + 
                          BsmtFin.SF.2  + Bsmt.Unf.SF +
                          X1st.Flr.SF   + X2nd.Flr.SF + 
                          Bedroom.AbvGr + Kitchen.AbvGr + 
                          Kitchen.Qual  + TotRms.AbvGrd + 
                          Garage.Type   + Garage.Area + 
                          Pool.Area     + Pool.QC + 
                          Screen.Porch  + Sale.Condition)

# I removed the Pool features, since those might be useless

Final.Model.red <- as.formula(price ~ Lot.Area + 
                          MS.SubClass   + Overall.Qual + 
                          Overall.Cond  + Year.Built + 
                          Mas.Vnr.Type  + Mas.Vnr.Area + 
                          Exter.Qual    + Exter.Cond + 
                          Bsmt.Exposure + BsmtFin.SF.1 + 
                          BsmtFin.SF.2  + Bsmt.Unf.SF +
                          X1st.Flr.SF   + X2nd.Flr.SF + 
                          Bedroom.AbvGr + Kitchen.AbvGr + 
                          Kitchen.Qual  + TotRms.AbvGrd + 
                          Garage.Type   + Garage.Area + 
                          Screen.Porch  + Sale.Condition)
```


```{r}
model.BIC = bas.lm(Final.Model, 
                    prior = "BIC",
                    modelprior = uniform(),
                    data = df.train)

model.BIC.red = bas.lm(Final.Model.red, 
                    prior = "BIC",
                    modelprior = uniform(),
                    data = df.train)

par(mfrow = c(2, 2))
plot(model.BIC)
```


```{r}
predicted.BIC <- predict(model.BIC, df.test)
predicted.BIC.red <- predict(model.BIC.red, df.test)

rmse.bic <- rmse(df.test$price, predicted.BIC$Ybma)
rmse.bic.red <- rmse(df.test$price, predicted.BIC.red$Ybma)

RMSE <- data.frame(rbind(rmse.bic, rmse.bic.red))
colnames(RMSE) <- c('RMSE')

kable(RMSE[1:1], caption = 'RMSE')
```

The model selection procedure proposed the following model. The inclusion probabilities (plot) above indicates that all used parameters are significant. But while looking at the model I figured that the `Pool.Area` and `Pool.QC` features can't have much of a relevance, since most of the houses do not even own a pool. Therefore I removed them and analysed the RMSE again. There is an improvement of the RMSE while removing the pool features. So I decided to stick with the parameters listed under **Decision on Final Model selection**. 

Based on the calculated RMSE, I came to the following decision and therefore will continue with this model for further investigations:

**Decision on Final Model selection**
**price ~ Lot.Area + MS.SubClass + Overall.Qual + Overall.Cond + Year.Built +** **Mas.Vnr.Type + Mas.Vnr.Area + Exter.Qual + Exter.Cond + Bsmt.Exposure + BsmtFin.SF.1 +BsmtFin.SF.2 + Bsmt.Unf.SF + X1st.Flr.SF + X2nd.Flr.SF + Bedroom.AbvGr +** 
**Kitchen.AbvGr + Kitchen.Qual + TotRms.AbvGrd + Garage.Type + Garage.Area + Screen.Porch + Sale.Condition**

**Coefficient Summaries**
The summary outputs have been aggregated for convenience purposes. The section shows the marginal posterior mean, standard deviation and posterior inclusion probabilities obtained by BMA.

```{r}
model.bic.coef = coef(model.BIC.red)
interval   <- confint(model.bic.coef)
names <- c("post mean", "post sd", colnames(interval))
interval   <- cbind(model.bic.coef$postmean, model.bic.coef$postsd, interval)
colnames(interval) <- names
interval
```


### Section 3.2 Transformation


In the previous section we defined the Final Model, which we will use to apply transformations on certain features. We saw earlier that the data indicates a non linearity, as well as features not being normal distributed. This are the points we need to address.

The Residuals plot from section 2.3 indicated clearly a non linearity of the data. Non linearity can be compensated by applying quadratic, log or root transformation to the model.

```{r}
df.train <- df.train.NoOutliner

# Model Selection without transformation 
Final.Model.base <- as.formula(price ~ Lot.Area + 
                                 MS.SubClass   + Overall.Qual + 
                                 Overall.Cond  + Year.Built + 
                                 Mas.Vnr.Type  + Mas.Vnr.Area + 
                                 Exter.Qual    + Exter.Cond + 
                                 Bsmt.Exposure + BsmtFin.SF.1 + 
                                 BsmtFin.SF.2  + Bsmt.Unf.SF +
                                 X1st.Flr.SF   + X2nd.Flr.SF + 
                                 Bedroom.AbvGr + Kitchen.AbvGr + 
                                 Kitchen.Qual  + TotRms.AbvGrd + 
                                 Garage.Type   + Garage.Area + 
                                 Screen.Porch  + Sale.Condition)

# Model Selection with squared parameter transformation
Final.Model.sqr <- as.formula(price ~ 
                            Lot.Area      + I(Lot.Area^2) + 
                            MS.SubClass   + I(MS.SubClass^2) + 
                            Overall.Qual  + Overall.Cond + 
                            Year.Built    + Mas.Vnr.Type + 
                            Mas.Vnr.Area  + I(Mas.Vnr.Area^2) + 
                            Exter.Qual    + Exter.Cond    + 
                            Bsmt.Exposure + 
                            BsmtFin.SF.1  + I(BsmtFin.SF.1^2) + 
                            BsmtFin.SF.2  + I(BsmtFin.SF.2^2) + 
                            Bsmt.Unf.SF   + 
                            X1st.Flr.SF   + I(X1st.Flr.SF^2) +         
                            X2nd.Flr.SF   + I(X2nd.Flr.SF^2) +  
                            Bedroom.AbvGr + I(Bedroom.AbvGr^2) +
                            Kitchen.AbvGr + 
                            Kitchen.Qual  + TotRms.AbvGrd + 
                            Garage.Type   + Garage.Area + 
                            Screen.Porch  + I(Screen.Porch^2) + 
                            Sale.Condition)

# Model Selection with squared- and log transformed parameters
Final.Model.Sqr_and_Log <- as.formula(log(price) ~ 
                            log(Lot.Area) + I(Lot.Area^2) + 
                            MS.SubClass   + I(MS.SubClass^2) + 
                            Overall.Qual  + Overall.Cond + 
                            Year.Built    + Mas.Vnr.Type + 
                            Mas.Vnr.Area  + I(Mas.Vnr.Area^2) + 
                            Exter.Qual    + Exter.Cond    + 
                            Bsmt.Exposure + 
                            BsmtFin.SF.1  + I(BsmtFin.SF.1^2) + 
                            BsmtFin.SF.2  + I(BsmtFin.SF.2^2) + 
                            Bsmt.Unf.SF   + 
                            X1st.Flr.SF   + I(X1st.Flr.SF^2) +         
                            X2nd.Flr.SF   + I(X2nd.Flr.SF^2) +  
                            Bedroom.AbvGr + I(Bedroom.AbvGr^2) +
                            Kitchen.AbvGr + 
                            Kitchen.Qual  + TotRms.AbvGrd + 
                            Garage.Type   + Garage.Area + 
                            Screen.Porch  + I(Screen.Porch^2) + 
                            Sale.Condition)

# Bayesian Model Averaging
# on the final base model
model.bic.base =  bas.lm(Final.Model.base, 
                         df.train,
                         prior="BIC",
                         modelprior=uniform(),
                         method = "MCMC", 
                         MCMC.iterations = 10^6) 

# Bayesian Model Averaging 
# on the final squared model
model.bic.sqr =  bas.lm(Final.Model.sqr, 
                        df.train,
                        prior="BIC",
                        modelprior=uniform(),
                        method = "MCMC", 
                        MCMC.iterations = 10^6) 

# Bayesian Model Averaging
# on the final squared and log transformed model
model.bic.sqr_log =  bas.lm(Final.Model.Sqr_and_Log, 
                   df.train,
                   prior="BIC",
                   modelprior=uniform(),
                   method = "MCMC", 
                   MCMC.iterations = 10^6) 

par(mfrow = c(2, 2))
plot(model.bic.base, which = 1)
plot(model.bic.sqr, which = 1)
plot(model.bic.sqr_log, which = 1)

predicted.bic.base    <- predict(model.bic.base, df.test)
predicted.bic.sqr     <- predict(model.bic.sqr, df.test)
predicted.bic.sqr_log <- predict(model.bic.sqr_log, df.test)

prediction <- data.frame(cbind( predicted.bic.base$Ybma, predicted.bic.sqr$Ybma, predicted.bic.sqr_log$Ybma))
prediction <- cbind(prediction, df.test$price)
names(prediction) <- c( 'Base', 'SQR', 'SQL LOG', 'Price')

rmse.bic.base <- rmse(df.test$price, predicted.bic.base$Ybma)
rmse.bic.sqr <- rmse(df.test$price, predicted.bic.sqr$Ybma)
rmse.bic.sqr_log <- rmse(df.test$price, exp(predicted.bic.sqr_log$Ybma))

RMSE <- data.frame(rbind(rmse.bic.base, rmse.bic.sqr, rmse.bic.sqr_log))
colnames(RMSE) <- c('RMSE')

kable(RMSE[1:1], caption = 'RMSE on test data')
```

The graph above shows 3 plots. Plot 1 only uses the plain formula. Plot 2 has some quadratic elements attached, while plot 3 uses quadratic as well as logarithmic elements. These three residual plots show clearly how the regression line levels out (from bow shaped to a straight line) by using quadratic or logarithmic and quadratic functions.

Therefore, we will apply quadratic as well as logarithmic functions for some of the features. A Log transformation has been applied with the response variable `price` and the explanatory variable `Lot.Area` to get a normal distribution in the data, while the features `Lot.Area` / `MS.SubClass` / `Mas.Vnr.Area` / `BsmtFin.SF.1` / `BsmtFin.SF.2` / `X1st.Flr.SF` / `X2nd.Flr.SF` / `Bedroom.AbvGr` / `Screen.Porch` have been squared to overcome the non linearity. By reducing the non linearity we are able to reduce the RMSE of the model. The table above indicates that we were able to constantly reduce the RMSE by applying the quadratic and log functions. That is now exactly the time where we need to investigate a little further by using the validation dataset. I did that and came to the conclusion to use both aspects - quadratic and log functions. Results with the validation set will be discussed in section 4.1.

### Section 3.3 Variable Interaction

We now will investigate if we need to apply any interaction variables. For that purpose we will investigate on how much multicollinearity (correlation between predictors) exists in our regression model via Variance inflation factors (VIF). Multicollinearity is problematic because it can increase the variance of the regression coefficients, making them unstable and difficult to interpret. Variance inflation factors (VIF) measure how much the variance of the estimated regression coefficients are inflated as compared to when the predictor variables are not linearly related.


```{r}
df.train <- df.train.NoOutliner

Final.Model.base <- as.formula(price ~ Lot.Area + 
                                 MS.SubClass   + Overall.Qual + 
                                 Overall.Cond  + Year.Built + 
                                 Mas.Vnr.Type  + Mas.Vnr.Area + 
                                 Exter.Qual    + Exter.Cond + 
                                 Bsmt.Exposure + BsmtFin.SF.1 + 
                                 BsmtFin.SF.2  + Bsmt.Unf.SF +
                                 X1st.Flr.SF   + X2nd.Flr.SF + 
                                 Bedroom.AbvGr + Kitchen.AbvGr + 
                                 Kitchen.Qual  + TotRms.AbvGrd + 
                                 Garage.Type   + Garage.Area + 
                                 Screen.Porch  + Sale.Condition)

vif(lm(Final.Model.base, df.train))
```

The figures above indicates that all of our features are moderately correlated. Therefore, we decide **not to include any variable interactions**.

### Section 3.4 Variable Selection

I went through various steps to identify the final model. The method used is just one part of the story. Feature engineering, cleaning and recoding of the data is one of the major tasks on the route to select the best model. Having finalized the cleaning, I used multiple approaches to select the final model. The approaches are listed below:

1-Feature engineering
2-Model Selection
2-1) Bayesian Model Averaging (Bayesian Information Criterion)
2-2) Akaike information criterion (AIC)
2-3) Removed some features
3-Model Tuning
3-1) Apply quadratic / log transformation
4-Model validation
Each used approach came out with a different **best** model. So we needed compare the different models to finally identify the best model.

The final Model is the following:

**log(price) ~ log(Lot.Area) + I(Lot.Area^2) + MS.SubClass + I(MS.SubClass^2) +** **Overall.Qual + Overall.Cond + Year.Built + Mas.Vnr.Type + Mas.Vnr.Area +** **I(Mas.Vnr.Area^2) + Exter.Qual + Exter.Cond + Bsmt.Exposure + BsmtFin.SF.1 +** **I(BsmtFin.SF.1^2) + BsmtFin.SF.2 + I(BsmtFin.SF.2^2) + Bsmt.Unf.SF + X1st.Flr.SF +** **I(X1st.Flr.SF^2) + X2nd.Flr.SF + I(X2nd.Flr.SF^2) +**
**Bedroom.AbvGr + I(Bedroom.AbvGr^2) + Kitchen.AbvGr + Kitchen.Qual + TotRms.AbvGrd +** **Garage.Type + Garage.Area + Screen.Porch + I(Screen.Porch^2) + Sale.Condition**



### Section 3.5 Model Testing

The parsimonious model proposed in section 3.1 Final Model was used as initial model to run initial predictions on the test data set. The comparison of the RMSE by using both training- and test data, while predicting house prices indicated that the model was not overfitting.

Changes have taken place towards removing certain features (e.g. `Pool.Area` and `Pool.QC`), which proofed to be useless. I also went back to change features (e.g. `Sale.Condition` etc.), by clustering parameter values. Abnormal has been set to 1, while all others have been set to 0. This also improved the performance. Then I focused on the non - linearity of the model. I applied quadratic and log transformation to certain (e.g. `Lot.Area`, `BsmtFin.SF.1`, `X1st.Flr.SF` etc.) as indicated above. All this measures helped to improve the performance. 


The evaluation on the model has mainly been performed via the **Root Mean Squared Error**.


## Part 4 Final Model Assessment

### Section 4.1 Final Model Residual

```{r}
df.train <- df.train.NoOutliner

# Model Selection with squared and log transformation
Final.Model <- as.formula(log(price) ~ 
                            log(Lot.Area) + I(Lot.Area^2) + 
                            MS.SubClass   + I(MS.SubClass^2) + 
                            Overall.Qual  + Overall.Cond + 
                            Year.Built    + Mas.Vnr.Type + 
                            Mas.Vnr.Area  + I(Mas.Vnr.Area^2) + 
                            Exter.Qual    + Exter.Cond    + 
                            Bsmt.Exposure + 
                            BsmtFin.SF.1  + I(BsmtFin.SF.1^2) + 
                            BsmtFin.SF.2  + I(BsmtFin.SF.2^2) + 
                            Bsmt.Unf.SF   + 
                            X1st.Flr.SF   + I(X1st.Flr.SF^2) +         
                            X2nd.Flr.SF   + I(X2nd.Flr.SF^2) +  
                            Bedroom.AbvGr + I(Bedroom.AbvGr^2) +
                            Kitchen.AbvGr + 
                            Kitchen.Qual  + TotRms.AbvGrd + 
                            Garage.Type   + Garage.Area + 
                            Screen.Porch  + I(Screen.Porch^2) + 
                            Sale.Condition)

# Bayesian Model Averaging 
# on the final model
model.bic =  bas.lm(Final.Model, 
                        df.train,
                        prior="BIC",
                        modelprior=uniform(),
                        method = "MCMC", 
                        MCMC.iterations = 10^6) 

plot(model.bic, which = 1)
```

The plot has been shown already in section 3.2, where we investigated on the feature transformation. As one can see, the residuals plot shows a nearly straight regression line, which means that we were able to compensate the non linearity of our data. So far we managed to establish a decent model by introducing quadratic and log transformed features. There is certainly still potential in optimizing our model, by optimizing the strategy for using NA values or creating new features.


### Section 4.2 Final Model RMSE

We will now calculate the Root Mean Squared Error (RMSE) of our final model on the test dataset.

```{r}
df.train <- df.train.NoOutliner

# Model Selection with squared and log transformation
Final.Model <- as.formula(log(price) ~ 
                            log(Lot.Area) + I(Lot.Area^2) + 
                            MS.SubClass   + I(MS.SubClass^2) + 
                            Overall.Qual  + Overall.Cond + 
                            Year.Built    + Mas.Vnr.Type + 
                            Mas.Vnr.Area  + I(Mas.Vnr.Area^2) + 
                            Exter.Qual    + Exter.Cond    + 
                            Bsmt.Exposure + 
                            BsmtFin.SF.1  + I(BsmtFin.SF.1^2) + 
                            BsmtFin.SF.2  + I(BsmtFin.SF.2^2) + 
                            Bsmt.Unf.SF   + 
                            X1st.Flr.SF   + I(X1st.Flr.SF^2) +         
                            X2nd.Flr.SF   + I(X2nd.Flr.SF^2) +  
                            Bedroom.AbvGr + I(Bedroom.AbvGr^2) +
                            Kitchen.AbvGr + 
                            Kitchen.Qual  + TotRms.AbvGrd + 
                            Garage.Type   + Garage.Area + 
                            Screen.Porch  + I(Screen.Porch^2) + 
                            Sale.Condition)

# Bayesian Model Averaging
# on the final model
final.model.bic =  bas.lm(Final.Model, 
                   df.train,
                   prior="BIC",
                   modelprior=uniform(),
                   method = "MCMC", 
                   MCMC.iterations = 10^6) 

predicted.bic.final    <- predict(final.model.bic, df.test)
rmse.bic <- rmse(df.test$price, exp(predicted.bic.final$Ybma))

RMSE <- data.frame(rbind(rmse.bic))
colnames(RMSE) <- c('RMSE')

kable(RMSE[1:1], caption = 'RMSE on Test data')
```


The RMSE value we have with the current model needs to be considered in the context of the initial model we started with in section 2.4. There we had an RMSE of around **34'97**1 dollar on the training data, where the current model shows an RMSE of around **19'207** dollar on a predicted house (on test data). Comparing those figures we improved our model by almost 50%, which is quite substantial. A real live test with validation data, will give us the final confirmation if our model will fulfil it's duty. One thing to mention is the importance of feature engineering and model selection in the process of defining a model.

### Section 4.3 Final Model Evaluation

**Strengths of the final model**
1- I spent quite some time for the feature engineering part. Applied some cleaning of outliners and high leverage points (Cook distance). But it finally proved to be helpful to get a more robust model.
2-Model Selection has been performed towards the parsimonious model. The model with the lowest number of features has been used. But I also went a step further to remove certain features to reduce the model complexity.
3-Multicollinearity has been avoided by verifying features via Variance inflation factors.
4-Feature transformation has been applied to compensate the non linearity of the model
5-The model properly reflect uncertainty confirmed by determining the coverage probability and the true value of price that fall within the 95% prediction confidence interval is 97%
6-The final model has proofed to have the smallest RMSE during real life (validation) tests. The lower the RMSE the better the model.

**Weaknesses of Final Model**
1-Although I invested a lot of time in feature engineering, there is a lot more to get out of it. Especially the strategy of filling the missing data. One can apply a more sophisticated approach then I did.
2-Features, which are available could be clustered to be more informative (e.g. Sale Condition - Normal and Not normal)
3-I also believe there are some features, which I reckon have some relevance, but could not proof it (combination of features)

### Section 4.4 Final Model Validation

Testing our final model on a separate, validation data set is a great way to determine how our model will perform in real-life practice.

**Visual interpretation of Residuals, Leverage and Q-Q plot**

```{r}
df.train <- df.train.NoOutliner

# Model Selection 
Final.Model <- as.formula(log(price) ~ 
                            log(Lot.Area) + I(Lot.Area^2) + 
                            MS.SubClass   + I(MS.SubClass^2) + 
                            Overall.Qual  + Overall.Cond + 
                            Year.Built    + Mas.Vnr.Type + 
                            Mas.Vnr.Area  + I(Mas.Vnr.Area^2) + 
                            Exter.Qual    + Exter.Cond    + 
                            Bsmt.Exposure + 
                            BsmtFin.SF.1  + I(BsmtFin.SF.1^2) + 
                            BsmtFin.SF.2  + I(BsmtFin.SF.2^2) + 
                            Bsmt.Unf.SF   + 
                            X1st.Flr.SF   + I(X1st.Flr.SF^2) +         
                            X2nd.Flr.SF   + I(X2nd.Flr.SF^2) +  
                            Bedroom.AbvGr + I(Bedroom.AbvGr^2) +
                            Kitchen.AbvGr + 
                            Kitchen.Qual  + TotRms.AbvGrd + 
                            Garage.Type   + Garage.Area + 
                            Screen.Porch  + I(Screen.Porch^2) + 
                            Sale.Condition)

model.lm <- lm(Final.Model, df.train)

par(mfrow = c(2, 2))
plot(model.lm, col = 'steelblue')
```

Consulting the plots above again, we see that the **Residual vs. Fitted** plot shows almost a straight line. A Q-Q plot is a scatterplot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a line that's roughly straight. This is not entirely for our distribution. Therefore, our model still shows improvement potential. If we look at the  **Residuals vs Leverage**, which provides a visual interpretation of high leverage points via the Cook's distance, we see that all our data points lay within the boundaries. 

**Comparing RMSE with training-, test and validation data applied**

```{r}
# Predict and calculate RMSE on training data 
predicted.train <- predict(model.lm, df.train)
RMSE.train <- rmse(df.train$price, exp(predicted.train))

# Predict and calculate RMSE on test data 
predicted.test <- predict(model.lm, df.test)
RMSE.test <- rmse(df.test$price, exp(predicted.test))

# Predict and calculate RMSE on validation data 
predicted.validation <- predict(model.lm, df.validation)
RMSE.validation <- rmse(df.validation$price, exp(predicted.validation))

RMSE <- data.frame(rbind(RMSE.train, RMSE.test, RMSE.validation))
colnames(RMSE) <- c('RMSE')
kable(RMSE[1:1], caption = 'RMSE')
```

The final model evaluation states a quite nice picture as you can see from the table above. The Root Mean Squared Error (RMSE), which is at around **21'810** dollar, while calculating the RMSE based on training data. The RMSE slightly drops With the test data (dollar - 18'630), but using the real-life (validation) data, our model shows slight rise of the RMSE (dollar - 18'817). In general, the lower the RMSE, the better the model fit. Therefore, our final model is a better fitted for validation data. 

**Under- and Overvalued houses**
Which houses will be Under- and Overvalued by our model. To answer this question we need to have a look at the Residuals vs Fitted plot. The plot can be separated into 2 areas - lower 25% and medium to higher (75%) sections. Houses in the lower price segment (25%) will slightly be over valued. This is obvious, since we have a slight non linearity in this area. Housing prices in the mid and high range should be pretty precise, since we have almost a straight regression line. 

**Coverage of true values**

```{r}
final.prediction <- predict(model.lm, df.validation, interval = "prediction", level = 0.95)
coverage <- mean(df.validation$price > exp(final.prediction[,"lwr"]) & df.validation$price < exp(final.prediction[,"upr"]))
coverage
```

The uncertainty respectively the coverage has been calculated, to assess if the we met the 95% confidence interval containing the true value of the houses. The calculation of the 95% confidence interval reveals that the true value of housing prices is met in around 97% of the cases. That means, this final model properly reflects the uncertainty.

## Part 5 Conclusion

The data set at hand was used to gather deep insight on the key characteristics that can influence sales prices of a homes in Ames, Iowa. The stepwise approach in analysing data thoroughly, deriving information from it towards data transformation to reach a decent result at the end, is quite a journey. Feature engineering, cleaning and recoding of the data proofed to be one of the major tasks on the route to select the best model. I also experienced that this is one of the most time consuming parts in designing a model. The Model selection process by using multiple approaches gave me a deep insight in the diverse selection methods. To me it was helpful to start with an initial model to get a feeling, respectively get a reference point to start with. Constantly measuring the improvement of the model via **adjusted R22, Root Mean Squared Error** or visual representations as **Residual vs. Fitted** plots is vital to understand the behaviour of each change on the model. There was one 'aha' effect I had, when I went into investigating the impact of outliners and high leverage points. Removing 2 outliners from the whole data set changed the initial parsimonious model to a new parsimonious model. I didn't expect 2 data points having such an impact. My fundamental learning in this assessment was that getting the best model wasn't primarily driven by the model I used. It was rather driven by managing outliners, high leverage points, feature engineering and transformation. In those areas lies the real potential to get a good model. Following a stepwise approach (improving and constantly measuring) by using the mentioned approaches in cooperation with the Bayesian Model Averaging approach, we were able to find a robust model. If we compare where we started off with the initial model (RMSE: 34'971 Dollars - on training data) and where we ended (RMSE: 18'630 dollars on test data and 18'817 dollars on validation data), we were able to improve our model by close to 50%. It was a tremendous journey which gave me lots of learnings.


## Appendix

**Helper Functions**

The following functions were produced to fullfill certain recurring tasks, which have been used during this assessment.


```{r}

# An NA value usually means that there is no object. So if it is a numerical 
# variable we set the NA values to 0.
# Features (caracter variables) as Quality, Condition etc. often indicate items with an NA value if 
# such an object doesn't exist, so we replace the NA values by 'none'
#
# We also recode ordered factors as pseudo-continuous numerical variables
#
# Params: df    - data set to convert
#
# Return: df    - converted dataset
NaHandler <- function( df ){
  
  df <- data.frame(df)
  
  # Initiall we do it the easy way. 
  # An NA value usually means that there is no object, so we set the NA values to 0

  df <- df %>% mutate(Garage.Cars    = ifelse(is.na(Garage.Cars), 0, Garage.Cars))
  df <- df %>% mutate(Garage.Area    = ifelse(is.na(Garage.Area), 0, Garage.Area))
  df <- df %>% mutate(Bsmt.Full.Bath = ifelse(is.na(Bsmt.Full.Bath), 0, Bsmt.Full.Bath))
  df <- df %>% mutate(Bsmt.Half.Bath = ifelse(is.na(Bsmt.Half.Bath), 0, Bsmt.Half.Bath))
  df <- df %>% mutate(BsmtFin.SF.1   = ifelse(is.na(BsmtFin.SF.1), 0, BsmtFin.SF.1))
  df <- df %>% mutate(BsmtFin.SF.2   = ifelse(is.na(BsmtFin.SF.2), 0, BsmtFin.SF.2))
  df <- df %>% mutate(Bsmt.Unf.SF    = ifelse(is.na(Bsmt.Unf.SF), 0, Bsmt.Unf.SF))
  df <- df %>% mutate(Total.Bsmt.SF  = ifelse(is.na(Total.Bsmt.SF), 0, Total.Bsmt.SF))

  df <- df %>% mutate(Lot.Frontage   = ifelse(is.na(Lot.Frontage), 0, Lot.Frontage))
  df <- df %>% mutate(Garage.Yr.Blt  = ifelse(is.na(Garage.Yr.Blt), 0, Garage.Yr.Blt))
  df <- df %>% mutate(Mas.Vnr.Area   = ifelse(is.na(Mas.Vnr.Area), 0, Mas.Vnr.Area))
    
  # The features like Quality, Condition etc. often indicate items with an NA value if 
  # such an object doesn't exist, so we replace the NA values to 'none'
  
  # Recode ordered factors as pseudo-continuous numerical variables
  df$Bsmt.Qual      <- recode(df$Bsmt.Qual, 'none')
  df$Bsmt.Cond      <- recode(df$Bsmt.Cond, 'none')
  df$Fireplace.Qu   <- recode( df$Fireplace.Qu, 'none' )
  df$Pool.QC        <- recode( df$Pool.QC, 'none' )
  df$Bsmt.Qual      <- recode( df$Bsmt.Qual, 'none' )
  df$Bsmt.Cond      <- recode( df$Bsmt.Cond, 'none' )
  df$Bsmt.Exposure  <- recode( df$Bsmt.Exposure, 'none')
  df$BsmtFin.Type.1 <- recode( df$BsmtFin.Type.1, 'none')
  df$BsmtFin.Type.2 <- recode( df$BsmtFin.Type.2, 'none' )
  df$Fence          <- recode( df$Fence, 'none' )
  df$Alley          <- recode( df$Alley, 'none' )
  df$Garage.Type    <- recode( df$Garage.Type, 'none' )
  df$Garage.Qual    <- recode( df$Garage.Qual, 'none' )
  df$Garage.Finish  <- recode( df$Garage.Finish, 'none' )
  df$Garage.Cond    <- recode( df$Garage.Cond, 'none' )
  df$Misc.Feature   <- recode( df$Misc.Feature, 'none' )  
  df$Mas.Vnr.Type   <- recode( df$Mas.Vnr.Type, 'None' ) # 29
  
 # TODO
 # Year Built 
  
  return(df)
}
```


```{r}

# Map features to numerical values
#
# Params: df    - data set to convert
#
# Return: df    - converted dataset
recode.features <- function(df){
  
  # Map features to numerical values
  feature.list = c('Exter.Qual', 'Exter.Cond', 'Garage.Qual', 'Garage.Cond', 
                   'Fireplace.Qu', 'Kitchen.Qual', 'Heating.QC', 'Bsmt.Qual', 
                   'Bsmt.Cond', 'Pool.QC' )
  param.list = c('none' = 0, 'Po' = 1, 'Fa' = 2, 'TA' = 3, 'Gd' = 4, 'Ex' = 5)
  df <- map.fcn(feature.list, param.list, df)
  
  param.list = c('none' = 0, 'No' = 1, 'Mn' = 2, 'Av' = 3, 'Gd' = 4)
  data.train <- map.fcn(c('Bsmt.Exposure'), param.list, df)
  
  feature.list = c('BsmtFin.Type.1','BsmtFin.Type.2')
  param.list = c('none' = 0, 'Unf' = 1, 'LwQ' = 2,'Rec'= 3, 'BLQ' = 4, 
                 'ALQ' = 5, 'GLQ' = 6)
  df = map.fcn(feature.list, param.list, df)
  
  param.list = c('None' = 0, 'Sal' = 1, 'Sev' = 2, 'Maj2' = 3, 'Maj1' = 4, 
                 'Mod' = 5, 'Min2' = 6, 'Min1' = 7, 'Typ'= 8)
  df = map.fcn(c('Functional'), param.list, df)
  
  param.list = c('none' = 0,'Unf' = 1, 'RFn' = 2, 'Fin' = 3)
  df = map.fcn(c('Garage.Finish'), param.list, df)
  
  param.list = c('none' = 0, 'MnWw' = 1, 'GdWo' = 2, 'MnPrv' = 3, 
                 'GdPrv' = 4)
  df = map.fcn(c('Fence'), param.list, df)
  
  param.list = c('none' = 0, 'Shed' = 1, 'Othr' = 2, 'Gar2' = 3, 
                 'Elev' = 4, 'TenC' = 5)
  df = map.fcn(c('Misc.Feature'), param.list, df)
  
  param.list = c('none' = 0, 'CarPort' = 1, 'Detchd' = 2, 'BuiltIn' = 3, 
                 'Basment' = 4, 'Attchd' = 5, '2Types' = 6)
  df = map.fcn(c('Garage.Type'), param.list, df)
  
  param.list = c('None' = 0, 'Stone' = 1, 'CBlock' = 2, 'BrkFace' = 3, 
                 'BrkCmn' = 4)
  df = map.fcn(c('Mas.Vnr.Type'), param.list, df)
  
  param.list = c('none' = 0, 'Grvl' = 1, 'Pave' = 2)
  df = map.fcn(c('Alley'), param.list, df)

  param.list = c('none' = 0, 'ELO' = 1, 'NoSewr' = 2, 'NoSeWa' = 3, 
                 'AllPub' = 4)
  df = map.fcn(c('Utilities'), param.list, df)
  
  param.list = c('none' = 1, 'N' = 1, 'P' = 2, 'Y' = 3)
  df = map.fcn(c('Paved.Drive'), param.list, df)
  
  param.list = c('none' = 0, 'N' = 0,'Y' = 1)
  df = map.fcn(c('Central.Air'), param.list, df)

  param.list = c('none' = 1, 'Grvl' = 1,'Pave' = 2)
  df = map.fcn(c('Street'), param.list, df)

  param.list = c('none' = 0, 'No' = 1, 'Mn' = 2, 'Gd' = 3, 'Av' = 4)
  df = map.fcn(c('Bsmt.Exposure'), param.list, df)
  
  param.list = c('none' = 1, 'Inside' = 1, 'Corner' = 2, 'CulDSac' = 3, 
                 'CulDSac' = 4, 'FR2' = 5, 'FR3' = 6)
  df = map.fcn(c('Lot.Config'), param.list, df)
  
  param.list = c('none' = 1, 'A (all)' = 1, 'C (all)' = 2, 'RM' = 3, 'FV' = 4, 
                 'RH' = 5, 'RP' = 6, 'I (all)' = 7, 'RL' = 8)
  df = map.fcn(c('MS.Zoning'), param.list, df)
  
  param.list = c('none' = 1, 'Gtl' = 1, 'Mod' = 2, 'Sev' = 3)
  df = map.fcn(c('Land.Slope'), param.list, df)
  
  param.list = c('none' = 1, 'Reg' = 1, 'IR1' = 2, 'IR2' = 3, 'IR3' = 4)
  df = map.fcn(c('Lot.Shape'), param.list, df)
  
  param.list = c('none' = 1, 'Lvl' = 1, 'Low' = 2, 'HLS' = 3, 'Bnk' = 4)
  df = map.fcn(c('Land.Contour'), param.list, df)

  param.list = c('none' = 1, 'Normal' = 1, 'Partial' = 2, 'AdjLand' = 3, 
                 'Abnorml' = 4, 'Alloca' = 5, 'Family' = 6)
  df = map.fcn(c('Sale.Condition'), param.list, df)
  
  param.list = c('none' = 10, 'COD' = 1, 'Con' = 2, 'ConLD' = 3, 'ConLI' = 4, 
                 'ConLw' = 5, 'CWD' = 6, 'New' = 7, 'Oth' = 8, 'VWD' = 9, 'WD ' = 10)
  df = map.fcn(c('Sale.Type'), param.list, df)
  
  param.list = c('none' = 5, 'FuseA' = 1, 'FuseF' = 2, 'FuseP' = 3, 'Mix' = 4, 
                 'SBrkr' = 5)
  df = map.fcn(c('Electrical'), param.list, df)
  
  param.list = c('none' = 2, 'Floor' = 1, 'GasA' = 2, 'GasW' = 3, 'Grav' = 4, 
                 'OthW' = 5, 'Wall' = 6 )
  df = map.fcn(c('Heating'), param.list, df)
  
  param.list = c('none' = 3, 'BrkTil' = 1, 'CBlock' = 2, 'PConc' = 3, 'Slab' = 4, 
                 'Stone' = 5, 'Wood' = 6 )
  df = map.fcn(c('Foundation'), param.list, df)
  
  param.list = c('none' = 15, 'AsbShng' = 1, 'AsphShn' = 2, 'BrkComm' = 3, 
                 'BrkFace' = 4, 'CBlock' = 5, 'CemntBd' = 6, 'HdBoard' = 7, 
                 'ImStucc' = 8, 'MetalSd' = 9, 'Other' = 10, 'Plywood' = 11,
                 'PreCast' = 12, 'Stone' = 13, 'Stucco' = 14, 'VinylSd' = 15, 
                 'Wd Sdng' = 16, 'WdShing' = 17, 'Wd Shng' = 18)
  df <- map.fcn('Exterior.1st', param.list, df)
  
  param.list = c('none' = 15, 'AsbShng' = 1, 'AsphShn' = 2, 'Brk Cmn' = 3, 
                 'BrkFace' = 4, 'CBlock' = 5, 'CmentBd' = 6, 'HdBoard' = 7, 
                 'ImStucc' = 8, 'MetalSd' = 9, 'Other' = 10, 'Plywood' = 11,
                 'PreCast' = 12, 'Stone' = 13, 'Stucco' = 14, 'VinylSd' = 15, 
                 'Wd Sdng' = 16, 'Wd Shng' = 17
  )
  df <- map.fcn('Exterior.2nd', param.list, df)
  
  feature.list = c('Condition.1', 'Condition.2')
  param.list = c('none' = 3, 'Artery' = 1, 'Feedr' = 2, 'Norm' = 3, 'RRNn' = 4, 
                 'RRAn' = 5, 'PosN' = 6, 'PosA' = 7, 'RRNe' = 8, 'RRAe' = 9)
  df <- map.fcn(feature.list, param.list, df)
  
  param.list = c('none' = 2, 'Flat' = 1, 'Gable' = 2, 'Gambrel' = 3, 'Hip' = 4, 
                 'Mansard' = 5, 'Shed' = 6 )
  df = map.fcn(c('Roof.Style'), param.list, df)
  
  param.list = c('none' = 2, 'ClyTile' = 1, 'CompShg' = 2, 'Membran' = 3, 
                 'Metal' = 4, 'Roll' = 5, 'Tar&Grv' = 6, 'WdShake' = 7, 
                 'WdShngl' = 8)
  df = map.fcn(c('Roof.Matl'), param.list, df)
  
  param.list = c('none' = 1, '1Fam' = 1, '2fmCon' = 2, 'Duplex' = 3, 'TwnhsE' = 4, 
                 'TwnhsI' = 5, 'Twnhs' = 6)
  df = map.fcn(c('Bldg.Type'), param.list, df)
  
  param.list = c('none' = 1, '1Story' = 1, '1.5Fin' = 2, '1.5Unf' = 3, '2Story' = 4, 
                 '2.5Fin' = 5, '2.5Unf' = 6, 'SFoyer' = 7, 'SLvl' = 8)
  df = map.fcn(c('House.Style'), param.list, df)
  
  param.list = c('none' = 1, 'Blmngtn' = 1, 'Blueste' = 2, 'BrDale' = 3, 
                 'BrkSide' = 4, 'ClearCr' = 5, 'CollgCr' = 6, 'Crawfor' = 7, 
                 'Edwards' = 8, 'Gilbert' = 9, 'Greens' = 10, 'GrnHill' = 11,
                 'IDOTRR' = 12, 'Landmrk' = 13, 'MeadowV' = 14, 'Mitchel' = 15, 
                 'NAmes' = 16, 'NoRidge' = 17, 'NPkVill' = 18, 'NridgHt' = 19, 
                 'NWAmes' = 20, 'OldTown' = 21, 'SWISU' = 22, 'Sawyer' = 23,
                 'SawyerW' = 24, 'Somerst' = 25, 'StoneBr' = 26, 'Timber' = 27, 
                 'Veenker' = 28
  )
  df = map.fcn(c('Neighborhood'), param.list, df)
  
   
  return(df)
 }
```

```{r}

# function that maps a categorical values into its corresponding numeric value and 
# returns that column to the data frame
#
# Params: cols       - columns which need attention
#         map.param  - param to replace
#         df         - Data set to be changed   
#  
# Return: df         - changed data set   
map.fcn = function(cols, map.param, df){
  
  for (col in cols) {
    df[col] = as.character(df[,col])
    df[col] = as.numeric(map.param[df[,col]])
  }
  return(df)
}


# Function checks all features in the data set and calculates the number of 
# NA's 
#
# Params: df         - Data set to evaluate NA's   
#         show       - show results
# Return: df         - changed data set   
CheckNA <- function( df , show = 1) {
  
  na_count <- sapply(df, function(x) sum(is.na(x)))
  data.na_count <- data.frame(na_count)
  data.merged <- data.frame(cbind(c(row.names(data.na_count)), data.na_count[, 1]))
  colnames(data.merged) <- c('feature', 'No_NA')
  
  data.na <- data.merged[data.merged$No_NA != 0,]
  
  if (show == 0) { 
    
  }
  
  if(show == 1){
    data.na
  }
  
  if(show == 2){
    cScheme <- dim(data.na)[1]
    ggplot(data = data.na, aes(x = data.na$feature, y = data.na$No_NA, fill = feature)) +
      geom_bar(stat = "identity") +
      xlab("feature") + ylab("Number of NA") +
      ggtitle("Number of NA's in dataset") +
      theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
      geom_text(aes(label = No_NA), vjust = 1.6, color = "black", position = position_dodge(0.9), size = 3) + 
      guides(FALSE) +
      scale_fill_manual(values = colorRampPalette(c("gray","steelblue" ))(cScheme))
  }
}

# Function to clean empty fields by placing a deined value NA 
#
# Params: df         - Data set to clean    
#
# Return: df         - cleaned data set   

cleanEmptyFields <- function(df, term){

  feature.list <- colnames(df)
  for (feature in feature.list) {
    df[feature][df[feature] == ''] <- term
    }
  return(df)
}

# Function to create formula based on given data set 
# 
# Params: data       - Data set to evaluate formula   
#         label      - dependent label          
# Return: formula    - formula 
getFormula <- function(data = data, label = label) {
  
  # label Variable
  labelVar <- label
  
  # separate measure Variable y column from rest 
  groupVariables <- setdiff(colnames(data), list(label))
  
  # create formula for model 
  formula <- as.formula(paste(labelVar, paste(groupVariables, collapse = ' + '), sep = ' ~ '))
  
}

# Min/max normalization of numeric columns
#
# Params: x         - features to normalize   
# Return:           - normalized values
normalize <- function(x) {
  
  return((x - min(x)) / (max(x) - min(x)))
}


# dmode  - will retrieve the mode of a given desity function (data) 
# 
# Params: vector     - label 
# Return:            - mode of dictribution
dmode <- function(x) {
  den <- density(x, kernel = c("gaussian"))
  ( den$x[den$y == max(den$y)] )   
}  


# getFormula  - will retrieve the formula based on a data set
# 
# Params: data     - dataset
#         label    - label 
# Return:          - formula
getFormula <- function(data = data, label = label) {
  
  # label Variable
  labelVar <- label
  
  # separate measure Variable y column from rest 
  groupVariables <- setdiff(colnames(data), list(label))
  
  # create formula for model 
  formula <- as.formula(paste(labelVar, paste(groupVariables, collapse = ' + '), sep = ' ~ '))
  
}

# prepNormalized_Df  - Extact and remove column of dependent variable from data set. 
#                      Column will be reattached after normalization of data set
# 
# Params: data     - dataset
#         feature  - dependent variable
# Return: data     - normalized restructured dataset
NormalizeDF <- function(data, feature){

  # Extact and remove column of dependent variable from data set. Column will be reattached after 
  # normalization of data set
  depVariable <- data.frame(data[feature])
  data[feature] <- NULL
 
  # Normalize the data set and attach price column again
  df.normalized <- normalize(data)
  df.normalized  <- cbind(df.normalized, depVariable)

  return(df.normalized)
}

# RMSE  - Calcuates the Root Mean Squared Error
# 
# Params: actual     - actual values
#         predicted  - predicted values 
# Return: rmse       - Root Mean Squared Error

rmse <- function(actual, predicted) {
  
  return(sqrt( mean((actual - predicted)^2)))
}

# CheckForOutliners  - Checks for outliners in the data set
#                      via resid 
# 
# Params: df         - data set
#         model      - proposed model 
# Return: df         - cleaned dataset
CheckForOutliners <- function(data, model){
  
  # evaluate the house with the max residuals 
  idx_outliner <- which(abs(resid(model)) == max(abs(resid(model))))
  idx_outliner <- unname(idx_outliner)
  
  data <- data[-c(idx_outliner), ]
  
  return(data)
}

# removeOutliners  - Removes outliners in the data set
# 
# Params: df         - data set
#         outliners  - outliners to remove 
# Return: df         - cleaned dataset
removeOutliners <- function(df, outliners){
  
  # based on our analysis we'll remove the most vicious 
  # outliners
  df <- df[-outliners, ]

  return(df)
}


finalCleanUp <- function(df){
  
  # Parcel identification number (PID) is not a relevant predictor. It can only be used 
  # with city web site for parcel review.  
  df$PID <- NULL
  
  # Factors that earlier analyses didn't like and too much of a pain in the neck to keep
  df$Fence <- NULL
  df$Roof.Style <- NULL
  df$Heating <- NULL
  df$Utilities <- NULL

  return(df)
}

# gg_pairs color function for lower regression parts
# 
# Return: gglplot       - return regression plot
ggpairs_lower <- function(data, mapping, ...) { 
  ggplot(data = data, mapping = mapping) + 
    geom_point(col = 'steelblue') + 
    geom_smooth(method = loess, fill = "red", color = "red", ...) +
    geom_smooth(method = lm, fill = "blue", color = "blue", ...)
  
}

# gg_pairs color function for diag density parts
# 
# Return: gglplot       - return density plot
ggpairs_diag <- function(data, mapping, ...){
  ggplot(data = data, mapping = mapping) + 
    geom_density(size = 1, colour = 'darkgreen')
  
}
```

